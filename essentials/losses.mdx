# Loss Functions

The `distillKitPlus/components/loss.py` module provides various loss functions suitable for knowledge distillation, combining a standard task loss with a distillation-specific loss term. The primary function used by the `LogitsTrainer` is `compute_distillation_loss`.

## `compute_distillation_loss`

This function orchestrates the calculation of the final loss used during training.

```python
def compute_distillation_loss(
    student_logits: Tensor,
    teacher_logits: Tensor,
    original_loss: Tensor, # Loss from the student model's standard task head
    inputs: Dict[str, Any] = None,
    loss_type: str = "fkl",
    k: int = 100,
    alpha: float = 0.1,
    temperature: float = 2.0,
    **kwargs, # Includes loss_kwargs for specific loss types
) -> Tensor:
```

**Parameters:**

*   `student_logits`: Logits output by the student model.
*   `teacher_logits`: Logits output by the teacher model (or pre-computed).
*   `original_loss`: The original loss calculated by the student model (e.g., cross-entropy loss for language modeling).
*   `inputs`: The batch data, potentially containing labels (`labels`, `teacher_labels`) needed by some loss types.
*   `loss_type` (str): Specifies which distillation loss to compute. Options: `'fkl'`, `'kld'`, `'uld'`, `'multi-ot'`.
*   `k` (int): Parameter for `uld` and `multi-ot` losses (number of top logits).
*   `alpha` (float): Weighting factor. The final loss is \( \alpha \times \text{distillation_loss} + (1 - \alpha) \times \text{original_loss} \).
*   `temperature` (float): Temperature for softening distributions in KL/softmax calculations.
*   `**kwargs`: Catches additional arguments, notably `loss_kwargs` (a dictionary) which can contain parameters specific to certain loss types (e.g., `log_loss_weight`, `sikhorn_loss_weight` for `multi-ot`).

**Functionality:**

1.  Selects the appropriate distillation loss function based on `loss_type`.
2.  Calls the selected function (e.g., `forward_kl`, `uld_loss`) with the necessary parameters.
3.  Combines the returned distillation loss (`kd_loss`) with the `original_loss` using the `alpha` weighting factor.

## Available Distillation Loss Types (`loss_type`)

### `'fkl'` or `'kld'` (Forward KL Divergence)

Calls `forward_kl(student_logits, teacher_logits, temperature, inputs)`.

*   Calculates the Kullback-Leibler divergence between the teacher's and student's probability distributions: \( D_{KL}(P_{teacher} || P_{student}) \).
*   Logits are scaled by `temperature` before computing softmax probabilities.
*   Handles vocabulary size mismatches by padding/truncating teacher logits.
*   The final KL divergence value is scaled by \( \text{temperature}^2 \) and averaged over the batch and sequence length ( `reduction='batchmean'` in `F.kl_div`).

### `'uld'` (Unbiased Logit Distillation)

Calls `uld_loss(student_logits, teacher_logits, temperature, k, inputs, **kwargs)`.

*   **Requires:** `inputs` dictionary must contain `'labels'` and `'teacher_labels'` (with non-relevant tokens marked by `ignore_index`, default -100).
*   Aligns student and teacher sequences based on the valid (non-ignored) tokens identified by the labels. It only compares the overlapping valid parts.
*   Computes softmax probabilities (scaled by `temperature`) for the aligned logit spans.
*   Selects the `topk` probabilities from the vocabulary dimension for both student and teacher.
*   Pads the top-k values if `k` is larger than the model's vocabulary size.
*   Calculates the sum of element-wise L1 differences (absolute difference) between the student's and teacher's top-k probability vectors.
*   Averages this L1 difference across the valid sequence length for each sample.
*   Averages the resulting per-sample losses across the batch.

### `'multi-ot'` (Multi-Level Optimal Transport)

Calls `multi_level_ot_loss(student_logits, teacher_logits, temperature, k, inputs, **kwargs)`.

*   **Requires:** `inputs` dictionary must contain `'labels'` and `'teacher_labels'` (with non-relevant tokens marked by `ignore_index`, default -100).
*   **Requires:** `loss_kwargs` (passed via `compute_distillation_loss`'s `**kwargs`) should ideally contain `'log_loss_weight'` (float, default 0.1) and `'sikhorn_loss_weight'` (float, default 0.1).
*   Similar initial steps to `uld_loss`: aligns sequences based on labels, computes temperature-scaled softmax probabilities, selects `topk` probabilities.
*   **Key Difference:** It applies an additional sorting step (`sequence_level_sort_for_ot_loss`) to the top-k probabilities based on the sum of probabilities across the sequence dimension for each vocabulary index.
*   Calculates the L1 difference between the *sorted* top-k probabilities.
*   Calculates a weighted KL divergence (`KL_wo`) between the sorted top-k probabilities (using `log_loss_weight`).
*   Calculates a Sinkhorn loss (approximating EMD) between the sorted top-k probabilities using the `Sinkhorn_seq` class (using `sikhorn_loss_weight`).
*   The final loss for this type combines the masked L1 difference (averaged over valid length), the weighted KL term, and the weighted Sinkhorn term, then averaged across the batch. 
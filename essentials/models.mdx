# Model Loading and Configuration

The `distillKitPlus/components/models.py` module handles loading and configuring language models (both student and teacher) and their tokenizers. It uses the Hugging Face `transformers` and `peft` libraries.

## Core Functions

The module provides these key functions:

- `setup_tokenizer`: Initializes tokenizer with project settings
- `get_model_kwargs`: Builds arguments for model loading
- `load_base_model`: Loads the base transformer model
- `configure_lora`: Sets up LoRA configuration
- `load_adapter`: Loads pre-trained PEFT adapters
- `setup_model`: Handles loading a single model
- `load_models`: Main entry point for loading all models

## Quantization Support

DistilKitPlus supports 4-bit quantization using `bitsandbytes`:

```python
# Example configuration
if config.get("quantization", {}).get("enabled", False):
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch_dtype,
        bnb_4bit_use_double_quant=True
    )
    kwargs["quantization_config"] = quantization_config
```

Enable in config:
```json
{
  "quantization": {
    "enabled": true
  }
}
```

## LoRA Support

LoRA allows efficient distillation by training only a small set of parameters:

```json
{
  "lora": {
    "enable_training": true,
    "r": 16,
    "alpha": 32,
    "dropout": 0.05,
    "bias": "none",
    "task_type": "CAUSAL_LM",
    "target_modules": [
      "q_proj", "k_proj", "v_proj", "o_proj",
      "gate_proj", "up_proj", "down_proj"
    ]
  }
}
```

## Usage Example

```python
from distillKitPlus.components.models import load_models
import json

# Load configuration
with open("config/my_distillation_config.json", "r") as f:
    config = json.load(f)

# Load models
models_dict = load_models(config)

# Access components
student_model = models_dict["student_model"]
student_tokenizer = models_dict["student_tokenizer"]

# Optional teacher components
if "teacher_model" in models_dict:
    teacher_model = models_dict["teacher_model"]
    teacher_tokenizer = models_dict["teacher_tokenizer"]
```

## Best Practices

1. **Quantization**
   - Enable for models > 7B parameters
   - Use BF16 precision when possible

2. **LoRA**
   - Start with r=16 or r=32
   - Include attention and MLP layers

3. **Flash Attention**
   - Enable on A100/H100 GPUs
   - Check PyTorch/transformers compatibility

4. **Large Teachers**
   - Use pre-computed logits for large models
   - Record teacher vocabulary size
---
title: 'Quickstart'
description: 'Get started with DistilKitPlus'
---

## Installation

To use `DistilKitPlus`, you first need to clone the repository and install the necessary dependencies.

```bash
git clone https://github.com/agokrani/distillKitPlus.git # Updated URL
cd distillKitPlus
pip install -r requirements.txt
pip install -e . # Install the package in editable mode
```

Make sure you have the necessary dependencies installed, including `torch`, `transformers`, `trl`, `peft`, `datasets`, `accelerate`, `tensorflow` (for TFRecord loading), and potentially `deepspeed` if using it.

## Running a Distillation Job

The core process involves configuring and running the `distill_logits.py` script.

1.  **Configure:**
    *   Navigate to the `config/` directory.
    *   Make a copy of `default_config.json` (or another template like `config_online_qwq_phi4_uld.json`) and name it appropriately (e.g., `my_distillation_config.json`).
    *   Edit your new config file, adjusting parameters in each section (`dataset`, `models`, `tokenizer`, `training`, `distillation`, etc.) according to your specific needs. Refer to the [Configuration](./essentials/configuration) page for details on each parameter.
    *   **Crucially**, decide if you are using pre-computed logits or on-the-fly generation:
        *   **Pre-computed:** Set `dataset.logits_file` to the path of your `.tfrecord` file and ensure `models.teacher_vocab_size` is correctly specified.
        *   **On-the-fly:** Set `dataset.logits_file` to `null` and ensure `models.teacher` points to a valid teacher model identifier/path.

2.  **Run Locally:**
    *   Execute the `distill_logits.py` script from the root directory of the repository, pointing it to your configuration file:

    ```bash
    # Example using default config
    python scripts/local/distill_logits.py --config config/default_config.json

    # Example using a custom config
    python scripts/local/distill_logits.py --config config/my_distillation_config.json
    ```

3.  **Run with Accelerate (Distributed Training):**
    *   Ensure `execution.use_accelerate` is `true` in your config file.
    *   Optionally, specify an Accelerate config file path in `execution.accelerate_config` (refer to `accelerate_configs/` for examples) or configure Accelerate using `accelerate config`.
    *   Launch the script using `accelerate launch`:

    ```bash
    accelerate launch scripts/local/distill_logits.py --config config/my_distillation_config.json
    ```
    Accelerate will handle the distribution across multiple GPUs/nodes based on your Accelerate configuration.

4.  **Output:**
    *   Training logs will be printed to the console (and potentially logged to WandB if configured).
    *   Checkpoints will be saved periodically to subdirectories within the `training.output_dir` specified in your config.
    *   The final trained model (or adapter) will be saved to `training.output_dir / "final-distilled-checkpoint"`.

## Generating Teacher Logits (Optional)

If you prefer to pre-compute teacher logits instead of generating them on-the-fly during distillation, you can use the `generate_logits.py` script.

1.  **Configure:** This script likely uses a similar configuration file structure. Ensure the `models.teacher` and relevant `dataset` parameters are set correctly.
2.  **Run:** Execute the script, potentially pointing to a specific config:

    ```bash
    # Example (command might vary slightly based on the script's implementation)
    python scripts/local/generate_logits.py --config config/my_teacher_config.json --output_file generated_logits.tfrecord
    ```

    This will process the specified dataset using the teacher model and save the output logits to the specified TFRecord file (e.g., `generated_logits.tfrecord`). You can then use this path in the `dataset.logits_file` parameter of your main distillation config.
